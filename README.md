# ðŸš€ Retail ETL Pipeline Project  
### *PySpark | Hadoop HDFS | PostgreSQL | Docker | SQL Analytics*

This project demonstrates a complete end-to-end **ETL pipeline** built using **PySpark**, **Hadoop (HDFS)**, and **PostgreSQL**, all running in a **Dockerized environment**. It cleans raw retail data, stores it in a distributed file system (HDFS), and loads it into a PostgreSQL database for deriving actionable **business insights**.

> ðŸ§  Ideal for showcasing your **data engineering skills** and **analytical thinking** in interviews!

---

## ðŸ“š Table of Contents
- [ðŸ§± Architecture Overview](#-architecture-overview)
- [ðŸ›  Tools & Technologies](#-tools--technologies)
- [ðŸ“ Project Structure](#-project-structure)
- [ðŸ”„ ETL Workflow](#-etl-workflow)
- [ðŸ“Š Business Insights (SQL)](#-business-insights-sql)
- [ðŸ“ˆ Future Enhancements](#-future-enhancements)
- [âœ… Conclusion](#-conclusion)

---

## ðŸ§± Architecture Overview

```plaintext
Raw CSV File (local)
   â”‚
   â”œâ”€â”€ âœ… Cleaned using PySpark (Data_cleaning.py)
   â”‚
   â”œâ”€â”€ â¬†ï¸ Manually uploaded to HDFS (`hdfs dfs -put`)
   â”‚
   â”œâ”€â”€ ðŸ“‚ Merged output files into one cleaned CSV
   â”‚
   â””â”€â”€ ðŸ›¢ï¸ Loaded into PostgreSQL via Pandas & SQLAlchemy (conn.py)
```

---

## ðŸ›  Tools & Technologies

| Tool             | Purpose                            |
|------------------|-------------------------------------|
| PySpark          | Data cleaning & transformation      |
| Hadoop HDFS      | Distributed file storage            |
| PostgreSQL       | Data warehousing & analytics        |
| Pandas + SQLAlchemy | PostgreSQL integration in Python |
| Docker & Compose | Environment setup & isolation       |
| Git              | Version control                     |

---

## ðŸ“ Project Structure

```
etl-pipeline/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ output/
â”‚       â””â”€â”€ cleaned_data.csv         # Final merged clean file
â”œâ”€â”€ spark_scripts/
â”‚   â”œâ”€â”€ Data_cleaning.py             # PySpark data cleaner
â”‚   â”œâ”€â”€ conn.py                      # Upload to PostgreSQL
â”‚   â””â”€â”€ .env                         # DB credentials (not in Git)
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml           # Hadoop + PostgreSQL services
â””â”€â”€ README.md                        # You're reading it :)
```

---

## ðŸ”„ ETL Workflow

### ðŸ”¹ Step 1: Clean Data with PySpark

- Read raw CSV.
- Remove nulls, duplicates.
- Compute `TotalPrice = Quantity * UnitPrice`.
- Output saved as multiple `part-*.csv` files.

---

### ðŸ”¹ Step 2: Merge Output into Single CSV

```bash
head -n 1 part-00000-*.csv > cleaned_data.csv
tail -n +2 -q part-0000*.csv >> cleaned_data.csv
```

---

### ðŸ”¹ Step 3: Start Dockerized Environment

```bash
cd docker/
docker-compose up -d --build
```

---

### ðŸ”¹ Step 4: Upload to HDFS (Manual)

```bash
docker exec -it namenode bash
hdfs dfs -mkdir /retail
hdfs dfs -put /data/output/cleaned_data.csv /retail/
```

---

### ðŸ”¹ Step 5: Load into PostgreSQL

Update `.env`:

```env
POSTGRES_USER=retail_user
POSTGRES_PASSWORD=your_password
POSTGRES_DB=retail_db
```

Then run:

```bash
python3 spark_scripts/conn.py
```

---

## ðŸ“Š Business Insights (SQL)

âœ… **Top 5 Countries by Total Sales**
```sql
SELECT "Country", ROUND(SUM("TotalPrice")::numeric, 2) AS total_sales
FROM retail_data_cleaned
GROUP BY "Country"
ORDER BY total_sales DESC
LIMIT 5;
```

âœ… **Monthly Revenue Trend**
```sql
SELECT DATE_TRUNC('month', "InvoiceDate"::timestamp) AS month,
       ROUND(SUM("TotalPrice")::numeric, 2) AS monthly_revenue
FROM retail_data_cleaned
GROUP BY month
ORDER BY month;
```

âœ… **Top 10 Most Sold Products**
```sql
SELECT "Description", SUM("Quantity") AS total_quantity_sold
FROM retail_data_cleaned
GROUP BY "Description"
ORDER BY total_quantity_sold DESC
LIMIT 10;
```

âœ… **Top 5 Customers (Lifetime Value)**
```sql
SELECT "CustomerID", ROUND(SUM("TotalPrice")::numeric, 2) AS lifetime_value
FROM retail_data_cleaned
WHERE "CustomerID" IS NOT NULL
GROUP BY "CustomerID"
ORDER BY lifetime_value DESC
LIMIT 5;
```

âœ… **Average Order Value by Country**
```sql
SELECT "Country",
       ROUND(SUM("TotalPrice")::numeric / COUNT(DISTINCT "InvoiceNo")::numeric, 2) AS avg_order_value
FROM retail_data_cleaned
GROUP BY "Country"
ORDER BY avg_order_value DESC;
```

---

## ðŸ“ˆ Future Enhancements

- Automate Spark â†’ HDFS â†’ PostgreSQL using Spark jobs
- Add Apache Airflow for orchestration
- Visualize insights using Power BI, Tableau, or Dash
- Add unit tests for ETL validation
- Add CI/CD pipelines

---

## âœ… Conclusion

This project simulates a real-world **ETL + Analytics** use case using a modern data engineering stack. It demonstrates:

âœ… Hands-on experience with distributed processing  
âœ… Clear understanding of data movement from raw to insights  
âœ… Docker-based reproducibility  
âœ… Strong SQL analytics skills

> ðŸ’¼ **Definitely a great project to showcase in your resume or GitHub portfolio!**

---
